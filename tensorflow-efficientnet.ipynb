{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## TensorFlow Data Input Pipeline + AlexNet CNN \nThis notebook servers has a simple example of loading the dataset into TensorFlow for better processing and optimization.\nWe will process and visualize the dataset and later build a classification model on it. The dataset contains additional data such as segmentations and bounding boxes which are useful in bulding more robust models but we are not going to utilize that for this notebook.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-08-10T07:02:52.973444Z","iopub.status.busy":"2022-08-10T07:02:52.973057Z","iopub.status.idle":"2022-08-10T07:03:35.606369Z","shell.execute_reply":"2022-08-10T07:03:35.605362Z","shell.execute_reply.started":"2022-08-10T07:02:52.973396Z"},"papermill":{"duration":0.013837,"end_time":"2022-08-12T20:30:45.928985","exception":false,"start_time":"2022-08-12T20:30:45.915148","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"We get this note from the data description that some of the DICOM files are JPEG compressed. You may require additional resources to read the pixel array of these files, such as GDCM and pylibjpeg. WE will install this dependencies","metadata":{"papermill":{"duration":0.011733,"end_time":"2022-08-12T20:30:45.952773","exception":false,"start_time":"2022-08-12T20:30:45.941040","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import sys\n\n!{sys.executable} -m pip install '../input/cspine-helper/pylibjpeg-1.4.0-py3-none-any.whl' -q\n!{sys.executable} -m pip install '../input/cspine-helper/pylibjpeg_libjpeg-1.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' -q\n!{sys.executable} -m pip install '../input/cspine-helper/python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' -q","metadata":{"execution":{"iopub.status.busy":"2022-09-01T01:53:28.031105Z","iopub.execute_input":"2022-09-01T01:53:28.031743Z","iopub.status.idle":"2022-09-01T01:54:01.608043Z","shell.execute_reply.started":"2022-09-01T01:53:28.031702Z","shell.execute_reply":"2022-09-01T01:54:01.606641Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Handle imports","metadata":{"papermill":{"duration":0.012195,"end_time":"2022-08-12T20:32:17.001189","exception":false,"start_time":"2022-08-12T20:32:16.988994","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os \nimport pathlib\nimport glob \nfrom tqdm import tqdm \nimport gdcm\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pydicom","metadata":{"papermill":{"duration":5.884238,"end_time":"2022-08-12T20:32:22.897602","exception":false,"start_time":"2022-08-12T20:32:17.013364","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-09-01T01:54:23.034309Z","iopub.execute_input":"2022-09-01T01:54:23.034683Z","iopub.status.idle":"2022-09-01T01:54:28.966139Z","shell.execute_reply.started":"2022-09-01T01:54:23.034650Z","shell.execute_reply":"2022-09-01T01:54:28.965034Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sns.set()","metadata":{"papermill":{"duration":0.020938,"end_time":"2022-08-12T20:32:22.931034","exception":false,"start_time":"2022-08-12T20:32:22.910096","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parameters\nEPOCHS = 10\nBATCH_SIZE = 16\nIMAGE_SIZE = (512, 512)\nSEED = 42","metadata":{"papermill":{"duration":0.020831,"end_time":"2022-08-12T20:32:22.964467","exception":false,"start_time":"2022-08-12T20:32:22.943636","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set seed\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"papermill":{"duration":0.019993,"end_time":"2022-08-12T20:32:22.997139","exception":false,"start_time":"2022-08-12T20:32:22.977146","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data  EDA and Processing","metadata":{"papermill":{"duration":0.012635,"end_time":"2022-08-12T20:32:23.022243","exception":false,"start_time":"2022-08-12T20:32:23.009608","status":"completed"},"tags":[]}},{"cell_type":"code","source":" # the input root folder \nDATA_DIR = \"../input/rsna-2022-cervical-spine-fracture-detection/\"","metadata":{"papermill":{"duration":0.019429,"end_time":"2022-08-12T20:32:23.054535","exception":false,"start_time":"2022-08-12T20:32:23.035106","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets list the contents inside the root folder\nos.listdir(DATA_DIR)","metadata":{"papermill":{"duration":0.02532,"end_time":"2022-08-12T20:32:23.091668","exception":false,"start_time":"2022-08-12T20:32:23.066348","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at what is in the train.csv\ntrain_df = pd.read_csv(DATA_DIR + \"train.csv\")\ntrain_df.head(10)","metadata":{"papermill":{"duration":0.045176,"end_time":"2022-08-12T20:32:23.148988","exception":false,"start_time":"2022-08-12T20:32:23.103812","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.size","metadata":{"papermill":{"duration":0.022553,"end_time":"2022-08-12T20:32:23.184447","exception":false,"start_time":"2022-08-12T20:32:23.161894","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how many unique study instances do we have\ntrain_df.StudyInstanceUID.nunique()","metadata":{"papermill":{"duration":0.028161,"end_time":"2022-08-12T20:32:23.225484","exception":false,"start_time":"2022-08-12T20:32:23.197323","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train dataframe has metadata for the each study instance, c1..c7 are the cervical vertebrae planes and the values in the rows states whether it is fractured or not.\n\nWe will use this for our classification model.","metadata":{"papermill":{"duration":0.012607,"end_time":"2022-08-12T20:32:23.250564","exception":false,"start_time":"2022-08-12T20:32:23.237957","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# a little deeper inside the train images\nos.listdir(DATA_DIR + \"train_images\")[:5]","metadata":{"papermill":{"duration":0.115425,"end_time":"2022-08-12T20:32:23.378990","exception":false,"start_time":"2022-08-12T20:32:23.263565","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see that the train images folder has other subfolders with the study id has its name. Each study can contain several instances with several frames and for this case slices in dicom format.","metadata":{"papermill":{"duration":0.013388,"end_time":"2022-08-12T20:32:23.405237","exception":false,"start_time":"2022-08-12T20:32:23.391849","status":"completed"},"tags":[]}},{"cell_type":"code","source":"study_instance = \"1.2.826.0.1.3680043.17625\"\n# list the first 5 frames in a select study instance\nos.listdir(DATA_DIR + f\"train_images/{study_instance}\")[:5]","metadata":{"papermill":{"duration":0.04653,"end_time":"2022-08-12T20:32:23.464666","exception":false,"start_time":"2022-08-12T20:32:23.418136","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select all the dicom files in the study instance\nimg_list = glob.glob(DATA_DIR + f\"/train_images/{study_instance}/*.dcm\")\nlen(img_list)","metadata":{"papermill":{"duration":0.024031,"end_time":"2022-08-12T20:32:23.502566","exception":false,"start_time":"2022-08-12T20:32:23.478535","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loading Functionalities ","metadata":{"execution":{"iopub.execute_input":"2022-08-10T07:50:38.109684Z","iopub.status.busy":"2022-08-10T07:50:38.108624Z","iopub.status.idle":"2022-08-10T07:50:38.114374Z","shell.execute_reply":"2022-08-10T07:50:38.113244Z","shell.execute_reply.started":"2022-08-10T07:50:38.109633Z"},"papermill":{"duration":0.013597,"end_time":"2022-08-12T20:32:23.529823","exception":false,"start_time":"2022-08-12T20:32:23.516226","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def load_dicom(path):\n    \"\"\"\n    reads a dicom file and loads the image array inside it\n    inputs:\n        path: the path of the required dicom file\n    returns:\n        data: image pixel arrays\n    \"\"\"\n    img=pydicom.dcmread(path)\n    data=img.pixel_array\n    data=data-np.min(data)\n    if np.max(data) != 0:\n        data=data/np.max(data)\n    data=(data*255).astype(np.uint8)\n    return data","metadata":{"papermill":{"duration":0.022385,"end_time":"2022-08-12T20:32:23.565805","exception":false,"start_time":"2022-08-12T20:32:23.543420","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_generator():\n    \"\"\"\n    a function that will load the dataset from a list of image paths\n    \"\"\"\n    for path in img_list:\n        data = load_dicom(path)\n        yield data  # return the data has generator","metadata":{"papermill":{"duration":0.020799,"end_time":"2022-08-12T20:32:23.599590","exception":false,"start_time":"2022-08-12T20:32:23.578791","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets define a tensorflow dataset variable that will use the generator to get the image data\n# this is efficient beacuse it will only load the data into memory when needed\ntrain_dataset = tf.data.Dataset.from_generator(data_generator, (tf.uint8))","metadata":{"papermill":{"duration":3.136097,"end_time":"2022-08-12T20:32:26.749139","exception":false,"start_time":"2022-08-12T20:32:23.613042","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a quick look of the dataset contents\nfor i in train_dataset.take(1):\n    print(i.shape)\n    print(type(i))","metadata":{"papermill":{"duration":0.115261,"end_time":"2022-08-12T20:32:26.878157","exception":false,"start_time":"2022-08-12T20:32:26.762896","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Visualization","metadata":{"execution":{"iopub.execute_input":"2022-08-10T07:55:55.634222Z","iopub.status.busy":"2022-08-10T07:55:55.633855Z","iopub.status.idle":"2022-08-10T07:55:55.639198Z","shell.execute_reply":"2022-08-10T07:55:55.637968Z","shell.execute_reply.started":"2022-08-10T07:55:55.634190Z"},"papermill":{"duration":0.013419,"end_time":"2022-08-12T20:32:26.905259","exception":false,"start_time":"2022-08-12T20:32:26.891840","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def show_single(img, cmap=\"gray\"):\n    \"\"\"\n    plots a single image\n    \"\"\"\n    plt.imshow(img, cmap=cmap)\n    plt.axis(\"off\")","metadata":{"papermill":{"duration":0.022081,"end_time":"2022-08-12T20:32:26.940993","exception":false,"start_time":"2022-08-12T20:32:26.918912","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_single(i)","metadata":{"papermill":{"duration":0.236955,"end_time":"2022-08-12T20:32:27.192174","exception":false,"start_time":"2022-08-12T20:32:26.955219","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_batch(cmap=\"gray\"):\n    \"\"\"\n    visualizes a batch of images\n    \"\"\"\n    plt.figure(figsize=(16, 12))\n    for i, img in enumerate(train_dataset.take(20)):  # iterate through the dataset\n        plt.subplot(4, 5, i+1)\n        show_single(img, cmap=cmap)\n    plt.show()","metadata":{"papermill":{"duration":0.023788,"end_time":"2022-08-12T20:32:27.230565","exception":false,"start_time":"2022-08-12T20:32:27.206777","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### A look of the images using different color maps","metadata":{"papermill":{"duration":0.013741,"end_time":"2022-08-12T20:32:27.258700","exception":false,"start_time":"2022-08-12T20:32:27.244959","status":"completed"},"tags":[]}},{"cell_type":"code","source":"show_batch(cmap=\"gray\")","metadata":{"papermill":{"duration":2.252079,"end_time":"2022-08-12T20:32:29.525051","exception":false,"start_time":"2022-08-12T20:32:27.272972","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_batch(cmap=\"bone\")","metadata":{"papermill":{"duration":2.011198,"end_time":"2022-08-12T20:32:31.556663","exception":false,"start_time":"2022-08-12T20:32:29.545465","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_batch(cmap=\"inferno\")","metadata":{"papermill":{"duration":2.232335,"end_time":"2022-08-12T20:32:33.818178","exception":false,"start_time":"2022-08-12T20:32:31.585843","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have so far loaded the images, but it is not ready for training. We need to map the labels from the train_df and as seen earlier, we have 2019 unique study instances. Hence, we have to create a nested loop to iterate through all the study instances and the dicom files inside each study instance.","metadata":{"papermill":{"duration":0.039624,"end_time":"2022-08-12T20:32:33.895502","exception":false,"start_time":"2022-08-12T20:32:33.855878","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# lets modify the data generator, use 10 study instances\ndef data_generator():\n    for i, study_instance in enumerate(train_df.StudyInstanceUID[:5]):\n        for dcm in os.listdir(DATA_DIR + f\"train_images/{study_instance}\"):\n            train_labels = []\n            path = DATA_DIR + f\"train_images/{study_instance}/{dcm}\"\n            \n            img = load_dicom(path)\n            \n            # resize each image into a shape of (512, 512)\n            img = np.resize(img, (512, 512))\n            #  normalize image\n            img = img / 255.0\n            # convert from gray scale to rgb, this will be helpful incase we want to use pretrained models\n            img = tf.expand_dims(img, axis=-1)\n            img = tf.image.grayscale_to_rgb(img)\n            \n            train_labels.extend([\n                train_df.loc[i, \"C1\"],\n                train_df.loc[i, \"C2\"],\n                train_df.loc[i, \"C3\"],\n                train_df.loc[i, \"C4\"],\n                train_df.loc[i, \"C5\"],\n                train_df.loc[i, \"C6\"],\n                train_df.loc[i, \"C7\"],\n                train_df.loc[i, \"patient_overall\"] # end with patient overall\n            ])\n            yield img, train_labels","metadata":{"papermill":{"duration":0.053687,"end_time":"2022-08-12T20:32:33.989632","exception":false,"start_time":"2022-08-12T20:32:33.935945","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = tf.data.Dataset.from_generator(data_generator, (tf.float32, tf.int8))","metadata":{"papermill":{"duration":0.061435,"end_time":"2022-08-12T20:32:34.086230","exception":false,"start_time":"2022-08-12T20:32:34.024795","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img, label in train_data.take(1):\n    print(img.shape)\n    print(label.shape)\n    print(label)","metadata":{"papermill":{"duration":0.152126,"end_time":"2022-08-12T20:32:34.274623","exception":false,"start_time":"2022-08-12T20:32:34.122497","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the image once again\nshow_single(img, cmap=\"gray\")","metadata":{"papermill":{"duration":0.245695,"end_time":"2022-08-12T20:32:34.555539","exception":false,"start_time":"2022-08-12T20:32:34.309844","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have our one-hot encoded labels, the last thing is to prepare the dataset for training by batching, catching and shuffling","metadata":{"execution":{"iopub.execute_input":"2022-08-10T09:40:08.420004Z","iopub.status.busy":"2022-08-10T09:40:08.418329Z","iopub.status.idle":"2022-08-10T09:40:08.432983Z","shell.execute_reply":"2022-08-10T09:40:08.430658Z","shell.execute_reply.started":"2022-08-10T09:40:08.419921Z"},"papermill":{"duration":0.042343,"end_time":"2022-08-12T20:32:34.634717","exception":false,"start_time":"2022-08-12T20:32:34.592374","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Split into train and validation","metadata":{"papermill":{"duration":0.035611,"end_time":"2022-08-12T20:32:34.707551","exception":false,"start_time":"2022-08-12T20:32:34.671940","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# we first need to know the number of data points we are dealing with\nimg_count = 0\nfor _, _ in enumerate(train_df.StudyInstanceUID[:5]):\n    for _ in os.listdir(DATA_DIR + f\"train_images/{study_instance}\"):\n        img_count += 1\nprint(img_count)","metadata":{"papermill":{"duration":0.050362,"end_time":"2022-08-12T20:32:34.793618","exception":false,"start_time":"2022-08-12T20:32:34.743256","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_size = int(img_count * 0.2)\ntrain_data = train_data.skip(val_size)\nval_data = train_data.take(val_size)","metadata":{"papermill":{"duration":0.050276,"end_time":"2022-08-12T20:32:34.879160","exception":false,"start_time":"2022-08-12T20:32:34.828884","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def configure_for_performance(data):\n    data = data.cache()\n#     data = data.shuffle(buffer_size=300)\n    data = data.batch(16)\n    data = data.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return data","metadata":{"papermill":{"duration":0.046161,"end_time":"2022-08-12T20:32:34.997970","exception":false,"start_time":"2022-08-12T20:32:34.951809","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = configure_for_performance(train_data)\nval_data = configure_for_performance(val_data)","metadata":{"papermill":{"duration":0.049762,"end_time":"2022-08-12T20:32:35.082982","exception":false,"start_time":"2022-08-12T20:32:35.033220","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{"papermill":{"duration":0.036858,"end_time":"2022-08-12T20:32:35.155214","exception":false,"start_time":"2022-08-12T20:32:35.118356","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Flatten","metadata":{"papermill":{"duration":0.956633,"end_time":"2022-08-12T20:32:36.149184","exception":false,"start_time":"2022-08-12T20:32:35.192551","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Alex Net model\ndef alex_net():\n    model = Sequential()\n\n    # 1st Convolutional Layer\n    model.add(Conv2D(filters=96, input_shape=(512,512,3), kernel_size=(11,11),\\\n     strides=(4,4), padding='valid', activation=\"relu\"))\n    # Pooling \n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n    # Batch Normalisation before passing it to the next layer\n    model.add(BatchNormalization())\n\n    # 2nd Convolutional Layer\n    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid', activation=\"relu\"))\n    # Pooling\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 3rd Convolutional Layer\n    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation=\"relu\"))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 4th Convolutional Layer\n    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation=\"relu\"))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 5th Convolutional Layer\n    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid', activation=\"relu\"))\n    # Pooling\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # Passing it to a dense layer\n    model.add(Flatten())\n    # 1st Dense Layer\n    model.add(Dense(4096, input_shape=(512*512*3,), activation=\"relu\"))\n    # Add Dropout to prevent overfitting\n    model.add(Dropout(0.4))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 2nd Dense Layer\n    model.add(Dense(4096, activation=\"relu\"))\n    # Add Dropout\n    model.add(Dropout(0.4))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 3rd Dense Layer\n    model.add(Dense(1000, activation=\"relu\"))\n    # Add Dropout\n    model.add(Dropout(0.4))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # Output Layer with 8 probability classes\n    model.add(Dense(8, activation=\"softmax\"))\n    return model","metadata":{"papermill":{"duration":0.055531,"end_time":"2022-08-12T20:32:36.241022","exception":false,"start_time":"2022-08-12T20:32:36.185491","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = alex_net()","metadata":{"papermill":{"duration":0.244007,"end_time":"2022-08-12T20:32:36.521017","exception":false,"start_time":"2022-08-12T20:32:36.277010","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"papermill":{"duration":0.051399,"end_time":"2022-08-12T20:32:36.608413","exception":false,"start_time":"2022-08-12T20:32:36.557014","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(), \n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=[tf.keras.metrics.CategoricalAccuracy()]\n             )","metadata":{"papermill":{"duration":0.056889,"end_time":"2022-08-12T20:32:36.702411","exception":false,"start_time":"2022-08-12T20:32:36.645522","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training\nhistory = model.fit(train_data, validation_data=val_data,\n                   epochs=EPOCHS)","metadata":{"papermill":{"duration":167.475668,"end_time":"2022-08-12T20:35:24.213636","exception":false,"start_time":"2022-08-12T20:32:36.737968","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize training \ndef viz_loss(history):\n    train_loss = history[\"loss\"]\n    val_loss = history[\"val_loss\"]\n    iters = [i for i in range(EPOCHS)]\n    \n    plt.plot(iters, train_loss, label=\"Training Loss\")\n    plt.plot(iters, val_loss, label=\"Validation Loss\")\n    plt.title(\"A plot of Loss against number of iterations\")\n    plt.legend()\n    plt.show()\n    \ndef viz_acc(history):\n    train_loss = history[\"categorical_accuracy\"]\n    val_loss = history[\"val_categorical_accuracy\"]\n    iters = [i for i in range(EPOCHS)]\n    \n    plt.plot(iters, train_loss, label=\"Training Accuracy\")\n    plt.plot(iters, val_loss, label=\"Validation Accuracy\")\n    plt.title(\"A plot of Accuracy against number of iterations\")\n    plt.legend()\n    plt.show()","metadata":{"papermill":{"duration":0.096202,"end_time":"2022-08-12T20:35:24.399166","exception":false,"start_time":"2022-08-12T20:35:24.302964","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz_loss(history.history)\nviz_acc(history.history)","metadata":{"papermill":{"duration":0.765862,"end_time":"2022-08-12T20:35:25.249559","exception":false,"start_time":"2022-08-12T20:35:24.483697","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"execution":{"iopub.execute_input":"2022-08-10T12:59:07.521353Z","iopub.status.busy":"2022-08-10T12:59:07.520703Z","iopub.status.idle":"2022-08-10T12:59:07.528859Z","shell.execute_reply":"2022-08-10T12:59:07.527249Z","shell.execute_reply.started":"2022-08-10T12:59:07.521319Z"},"papermill":{"duration":0.085939,"end_time":"2022-08-12T20:35:25.422823","exception":false,"start_time":"2022-08-12T20:35:25.336884","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# prep test data for submission\ntest_df = pd.read_csv(DATA_DIR + \"test.csv\")\ntest_df.head()","metadata":{"papermill":{"duration":0.108915,"end_time":"2022-08-12T20:35:25.633150","exception":false,"start_time":"2022-08-12T20:35:25.524235","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global test_ids\ntest_ids = []\ndef test_data_generator():\n    for study_instance in os.listdir(DATA_DIR + f\"test_images\"):\n        for dcm in os.listdir(DATA_DIR + f\"test_images/{study_instance}\"):\n            path = DATA_DIR + f\"test_images/{study_instance}/{dcm}\"\n            img = load_dicom(path)\n            \n            # resize each image into a shape of (512, 512)\n            img = np.resize(img, (512, 512))\n            #  normalize image\n            img = tf.cast(img, tf.float32) / 255.0\n            # convert from gray scale to rgb, this will be helpful incase we want to use pretrained models\n            img = tf.expand_dims(img, axis=-1)\n            img = tf.image.grayscale_to_rgb(img)\n            test_ids.append(study_instance)\n            yield img","metadata":{"papermill":{"duration":0.097977,"end_time":"2022-08-12T20:35:25.818499","exception":false,"start_time":"2022-08-12T20:35:25.720522","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = tf.data.Dataset.from_generator(test_data_generator, tf.float32).batch(1)","metadata":{"papermill":{"duration":0.113921,"end_time":"2022-08-12T20:35:26.017109","exception":false,"start_time":"2022-08-12T20:35:25.903188","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\npreds = []\nfor img in tqdm(test_data):\n    preds.append(model.predict(img)[0])\npreds = np.array(preds)","metadata":{"papermill":{"duration":142.145371,"end_time":"2022-08-12T20:37:48.248867","exception":false,"start_time":"2022-08-12T20:35:26.103496","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(test_ids) == len(preds)","metadata":{"papermill":{"duration":0.127731,"end_time":"2022-08-12T20:37:48.497167","exception":false,"start_time":"2022-08-12T20:37:48.369436","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.DataFrame(columns = [\"StudyInstanceUID\", 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'patient_overall'])","metadata":{"papermill":{"duration":0.131498,"end_time":"2022-08-12T20:37:48.809065","exception":false,"start_time":"2022-08-12T20:37:48.677567","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(range(len(test_ids))):\n    result.loc[i, 'StudyInstanceUID'] = test_ids[i]\n    rows = preds[i].round(3)\n    result.loc[i, 'C1'] = rows[0]\n    result.loc[i, 'C2'] = rows[1]\n    result.loc[i, 'C3'] = rows[2]\n    result.loc[i, 'C4'] = rows[3]\n    result.loc[i, 'C5'] = rows[4]\n    result.loc[i, 'C6'] = rows[5]\n    result.loc[i, 'C7'] = rows[6]\n    result.loc[i, 'patient_overall'] = rows[7]","metadata":{"papermill":{"duration":1.505831,"end_time":"2022-08-12T20:37:50.434177","exception":false,"start_time":"2022-08-12T20:37:48.928346","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.head()","metadata":{"papermill":{"duration":0.13735,"end_time":"2022-08-12T20:37:50.694685","exception":false,"start_time":"2022-08-12T20:37:50.557335","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# means = result[['patient_overall', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']].mean().to_dict()\n# sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n# sample_submission.head()\n# sample_submission.to_csv(\"submission.csv\", index=\"false\")","metadata":{"papermill":{"duration":0.128197,"end_time":"2022-08-12T20:37:50.942451","exception":false,"start_time":"2022-08-12T20:37:50.814254","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#make submission corresponding to the submission format\n# idxs = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'patient_overall']\n# sub = []\n# for i in tqdm(range(len(test_ids))):\n#     for j, el in enumerate(idxs):\n#         sub.append([result.loc[i].StudyInstanceUID + f\"_{el}\", result.loc[0][j+1]])","metadata":{"papermill":{"duration":0.128235,"end_time":"2022-08-12T20:37:51.190435","exception":false,"start_time":"2022-08-12T20:37:51.062200","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"means = result[['patient_overall', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']].mean().to_dict()\nprint(means)","metadata":{"papermill":{"duration":0.144733,"end_time":"2022-08-12T20:37:51.454742","exception":false,"start_time":"2022-08-12T20:37:51.310009","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['fractured'] = test_df['prediction_type'].map(means)\ntest_df[['row_id','fractured']].to_csv('submission.csv', index=False, float_format='%.1g')","metadata":{"papermill":{"duration":0.138262,"end_time":"2022-08-12T20:37:51.712789","exception":false,"start_time":"2022-08-12T20:37:51.574527","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat submission.csv","metadata":{"papermill":{"duration":1.296286,"end_time":"2022-08-12T20:37:53.129586","exception":false,"start_time":"2022-08-12T20:37:51.833300","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion \nWe have seen how to process the dicom files and transform the dataset into tensorflow format. However, we didn't use all the dataset and the cpu memory was filling up easily. We could do some optimizations to train with all the images.\n\n## What to do next\n1. Handle JPEG dicom format correctly (had some few errors)\n2. Train will all the images\n3. Use segmentations and bounding boxes\n4. Perform cross validation","metadata":{"papermill":{"duration":0.128965,"end_time":"2022-08-12T20:37:53.380814","exception":false,"start_time":"2022-08-12T20:37:53.251849","status":"completed"},"tags":[]}}]}